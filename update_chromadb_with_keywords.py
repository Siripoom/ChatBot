#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï ChromaDB metadata ‡πÉ‡∏´‡πâ‡∏°‡∏µ keywords ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥
"""

import json
import chromadb
from pathlib import Path
from typing import Dict, List
from pythainlp import word_tokenize

def load_keywords_data(keywords_file: str) -> Dict:
    """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• keywords ‡∏à‡∏≤‡∏Å JSON"""
    with open(keywords_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def find_matching_file(doc_metadata: Dict, keywords_data: Dict) -> str:
    """
    ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö metadata ‡∏Ç‡∏≠‡∏á document

    Args:
        doc_metadata: metadata ‡∏Ç‡∏≠‡∏á document ‡∏à‡∏≤‡∏Å ChromaDB
        keywords_data: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• keywords ‡∏à‡∏≤‡∏Å JSON

    Returns:
        ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô ‡∏´‡∏£‡∏∑‡∏≠ None
    """
    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å metadata
    if 'source' in doc_metadata or 'filename' in doc_metadata:
        source = doc_metadata.get('source') or doc_metadata.get('filename')

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏´‡∏ô‡πÉ‡∏ô keywords_data
        for filename in keywords_data['files'].keys():
            # ‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
            if Path(filename).stem in source or source in filename:
                return filename

    return None

def extract_keywords_from_text(text: str, top_n: int = 20) -> List[str]:
    """
    ‡∏™‡∏Å‡∏±‡∏î keywords ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥

    Args:
        text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏Å‡∏±‡∏î keywords
        top_n: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô keywords ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

    Returns:
        List ‡∏Ç‡∏≠‡∏á keywords
    """
    from collections import Counter
    from pythainlp.corpus import thai_stopwords

    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥
    words = word_tokenize(text, engine='newmm')

    # ‡∏Å‡∏£‡∏≠‡∏á stopwords ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏™‡∏±‡πâ‡∏ô
    stopwords = thai_stopwords()
    filtered = [w for w in words if len(w) >= 2 and w not in stopwords]

    # ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡πÅ‡∏•‡∏∞‡∏´‡∏≤ top keywords
    counter = Counter(filtered)
    return [word for word, _ in counter.most_common(top_n)]

def update_chromadb_metadata(
    persist_directory: str = "./chroma_db",
    collection_name: str = "chatbot_knowledge",
    keywords_file: str = "./data/keywords_analysis.json"
):
    """
    ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï metadata ‡πÉ‡∏ô ChromaDB ‡πÉ‡∏´‡πâ‡∏°‡∏µ keywords

    Args:
        persist_directory: ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ChromaDB
        collection_name: ‡∏ä‡∏∑‡πà‡∏≠ collection
        keywords_file: ‡πÑ‡∏ü‡∏•‡πå keywords JSON
    """
    print("=" * 80)
    print("üîÑ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï ChromaDB Metadata ‡∏î‡πâ‡∏ß‡∏¢ Keywords")
    print("=" * 80)

    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• keywords
    print(f"\nüìñ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• keywords ‡∏à‡∏≤‡∏Å: {keywords_file}")
    keywords_data = load_keywords_data(keywords_file)
    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(keywords_data['files'])} ‡πÑ‡∏ü‡∏•‡πå")

    # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ChromaDB
    print(f"\nüíæ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ChromaDB: {persist_directory}")
    client = chromadb.PersistentClient(path=persist_directory)

    try:
        collection = client.get_collection(name=collection_name)
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ collection: {collection_name}")
        print(f"üìö ‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {collection.count()} documents")
    except Exception as e:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö collection: {e}")
        return

    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å collection
    print("\nüîç ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å ChromaDB...")
    all_docs = collection.get(include=['documents', 'metadatas', 'embeddings'])

    doc_ids = all_docs['ids']
    documents = all_docs['documents']
    metadatas = all_docs['metadatas']
    embeddings = all_docs['embeddings']

    print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(documents)} documents")

    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ document
    print("\nüìù ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï metadata...")
    updated_count = 0
    fallback_count = 0

    for i, (doc_id, doc_text, metadata) in enumerate(zip(doc_ids, documents, metadatas), 1):
        # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
        matched_file = find_matching_file(metadata, keywords_data)

        if matched_file:
            # ‡πÉ‡∏ä‡πâ keywords ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á
            file_data = keywords_data['files'][matched_file]
            top_keywords = [kw['word'] for kw in file_data['keywords'][:30]]

            # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata
            metadata['keywords'] = top_keywords
            metadata['keywords_source'] = matched_file
            metadata['keywords_method'] = 'file_match'

            print(f"  [{i}/{len(documents)}] ‚úÖ {doc_id[:50]}... ‚Üí {matched_file}")
            updated_count += 1
        else:
            # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏™‡∏Å‡∏±‡∏î keywords ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            doc_keywords = extract_keywords_from_text(doc_text, top_n=20)

            metadata['keywords'] = doc_keywords
            metadata['keywords_source'] = 'text_extraction'
            metadata['keywords_method'] = 'fallback'

            print(f"  [{i}/{len(documents)}] ‚ö†Ô∏è  {doc_id[:50]}... ‚Üí ‡∏™‡∏Å‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°")
            fallback_count += 1

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á keywords_text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BM25
        metadata['keywords_text'] = ' '.join(metadata['keywords'][:15])

    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ ChromaDB
    print(f"\nüíæ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ ChromaDB...")

    try:
        # ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ upsert ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ ChromaDB ‡πÑ‡∏°‡πà‡∏°‡∏µ update ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        collection.upsert(
            ids=doc_ids,
            documents=documents,
            metadatas=metadatas,
            embeddings=embeddings
        )
        print(f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    except Exception as e:
        print(f"‚ùå ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return

    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    print("\n" + "=" * 80)
    print("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï")
    print("=" * 80)
    print(f"üìù ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(documents)} documents")
    print(f"‚úÖ ‡πÉ‡∏ä‡πâ keywords ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå: {updated_count} documents")
    print(f"‚ö†Ô∏è  ‡∏™‡∏Å‡∏±‡∏î keywords ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {fallback_count} documents")
    print(f"üéØ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {updated_count + fallback_count} documents")

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á metadata
    print("\nüìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á metadata ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß:")
    print("-" * 80)

    sample_idx = 0
    print(f"Document ID: {doc_ids[sample_idx]}")
    print(f"Metadata: {json.dumps(metadatas[sample_idx], ensure_ascii=False, indent=2)}")
    print(f"Top Keywords: {', '.join(metadatas[sample_idx]['keywords'][:10])}")

    print("\n" + "=" * 80)
    print("‚ú® ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
    print("=" * 80)

if __name__ == "__main__":
    update_chromadb_metadata(
        persist_directory="./chroma_db",
        collection_name="chatbot_knowledge",
        keywords_file="./data/keywords_analysis.json"
    )
