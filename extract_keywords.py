#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á keywords ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .docx
"""

import os
import json
import re
from collections import Counter
from docx import Document
from pythainlp import word_tokenize
from pythainlp.corpus import thai_stopwords

# English stopwords
ENGLISH_STOPWORDS = {
    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has',
    'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was',
    'will', 'with', 'the', 'this', 'but', 'they', 'have', 'had', 'what',
    'when', 'where', 'who', 'which', 'why', 'how', 'can', 'could', 'would',
    'should', 'may', 'might', 'must', 'or', 'not', 'no', 'yes', 'do', 'does'
}

def read_docx(file_path):
    """‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .docx"""
    try:
        doc = Document(file_path)
        full_text = []

        # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å paragraphs
        for para in doc.paragraphs:
            if para.text.strip():
                full_text.append(para.text)

        # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    if cell.text.strip():
                        full_text.append(cell.text)

        return '\n'.join(full_text)
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return ""

def is_valid_keyword(word, thai_stopwords_set):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô keyword ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    """
    word = word.strip()

    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤‡∏á
    if not word or word.isspace():
        return False

    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
    if len(word) < 2:
        return False

    # ‡∏Å‡∏£‡∏≠‡∏á Thai stopwords
    if word in thai_stopwords_set:
        return False

    # ‡∏Å‡∏£‡∏≠‡∏á English stopwords (‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å)
    if word.lower() in ENGLISH_STOPWORDS:
        return False

    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö ‡πÄ‡∏ä‡πà‡∏ô (3-0-6), (Prerequisite), (None)
    if '(' in word or ')' in word:
        return False

    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    if all(c in '.,!?;:()[]{}""''‚Äî-\n\t /\\|' for c in word):
        return False

    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    if word.isdigit():
        return False

    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô 3-0-6, 2.1
    if re.match(r'^[\d\-\.]+$', word):
        return False

    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏™‡∏±‡πâ‡∏ô‡πÜ (1-2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£) ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
    if len(word) <= 2 and word.isalpha() and word.isascii():
        return False

    return True

def extract_keywords(text, min_word_length=2, top_n=50):
    """
    ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡∏™‡∏Å‡∏±‡∏î keywords ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

    Parameters:
    - text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    - min_word_length: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ñ‡∏≥‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏£‡∏±‡∏ö
    - top_n: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô keywords ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    """
    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ newmm engine (‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
    words = word_tokenize(text, engine='newmm')

    # ‡πÇ‡∏´‡∏•‡∏î Thai stopwords
    thai_stopwords_set = thai_stopwords()

    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥
    filtered_words = []
    for word in words:
        if is_valid_keyword(word, thai_stopwords_set):
            filtered_words.append(word.strip())

    # ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥
    word_freq = Counter(filtered_words)

    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ top keywords ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà
    return word_freq.most_common(top_n)

def process_all_documents(doc_dir):
    """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå"""
    results = {}

    # ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    files = [f for f in os.listdir(doc_dir) if f.endswith('.docx')]

    print(f"‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(files)} ‡πÑ‡∏ü‡∏•‡πå")
    print("="*80)

    for filename in sorted(files):
        file_path = os.path.join(doc_dir, filename)
        print(f"\nüìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {filename}")
        print("-"*80)

        # ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        text = read_docx(file_path)

        if not text:
            print(f"‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å {filename}")
            continue

        print(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {len(text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")

        # ‡∏™‡∏Å‡∏±‡∏î keywords
        keywords = extract_keywords(text, min_word_length=2, top_n=100)

        # ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        results[filename] = {
            'file_name': filename,
            'text_length': len(text),
            'total_keywords': len(keywords),
            'keywords': [
                {
                    'word': word,
                    'frequency': freq
                }
                for word, freq in keywords
            ],
            'top_20_keywords': [word for word, freq in keywords[:20]]
        }

        # ‡πÅ‡∏™‡∏î‡∏á top 20 keywords
        print(f"\nüîë Top 20 Keywords:")
        for i, (word, freq) in enumerate(keywords[:20], 1):
            print(f"  {i:2d}. {word:30s} ({freq:3d} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)")

        print("\n" + "="*80)

    return results

def save_results(results, output_file):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: {output_file}")

def create_summary(results):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
    print("\n" + "="*80)
    print("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    print("="*80)

    total_keywords = sum(len(r['keywords']) for r in results.values())

    print(f"\nüìà ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(results)} ‡πÑ‡∏ü‡∏•‡πå")
    print(f"üìà Keywords ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_keywords} ‡∏Ñ‡∏≥")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ keywords ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°
    all_keywords = Counter()
    for result in results.values():
        for kw in result['keywords']:
            all_keywords[kw['word']] += kw['frequency']

    print(f"\nüèÜ Top 30 Keywords ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå:")
    for i, (word, freq) in enumerate(all_keywords.most_common(30), 1):
        print(f"  {i:2d}. {word:30s} ({freq:4d} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)")

    return all_keywords

if __name__ == "__main__":
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path
    doc_dir = "/home/siripoom/chatbot/data/Doc"
    output_file = "/home/siripoom/chatbot/data/keywords_analysis.json"

    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥")

    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    results = process_all_documents(doc_dir)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ
    all_keywords = create_summary(results)

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏ß‡∏°
    summary_data = {
        'files': results,
        'summary': {
            'total_files': len(results),
            'total_unique_keywords': len(all_keywords),
            'overall_top_keywords': [
                {'word': word, 'frequency': freq}
                for word, freq in all_keywords.most_common(100)
            ]
        }
    }

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    save_results(summary_data, output_file)

    print("\n‚ú® ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•!")
