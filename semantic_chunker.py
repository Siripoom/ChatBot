#!/usr/bin/env python3
"""
Semantic Chunker - ‡πÅ‡∏¢‡∏Å text ‡∏ï‡∏≤‡∏° context ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
"""

import re
from typing import List, Dict
from pythainlp import sent_tokenize
from langchain_text_splitters import RecursiveCharacterTextSplitter


class SemanticChunker:
    """
    Advanced text chunker that splits based on semantic meaning and document structure
    """

    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def detect_headings(self, text: str) -> List[Dict[str, any]]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        """
        headings = []

        # Pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ (‡πÄ‡∏ä‡πà‡∏ô "1.1", "1.1.1", "‡∏Å)", "‡∏Ç)", etc.)
        patterns = [
            # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô 1.1, 1.1.1, 1.1.1.1
            r'^(\d+\.(?:\d+\.)*\d*)\s+(.+?)$',
            # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å), ‡∏Ç), ‡∏Ñ)
            r'^([‡∏Å-‡∏Æ]\))\s+(.+?)$',
            # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏ö‡∏ö‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö ‡πÄ‡∏ä‡πà‡∏ô (1), (2)
            r'^\((\d+)\)\s+(.+?)$',
            # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ö‡∏ó ‡πÄ‡∏ä‡πà‡∏ô "‡∏ö‡∏ó‡∏ó‡∏µ‡πà 1"
            r'^(‡∏ö‡∏ó‡∏ó‡∏µ‡πà\s*\d+)\s*(.*)$',
            # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤ (‡∏≠‡∏≤‡∏à‡∏°‡∏µ marker)
            r'^(.+?)(?=\n{2,}|\Z)',
        ]

        lines = text.split('\n')
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue

            for pattern in patterns:
                match = re.match(pattern, line, re.MULTILINE)
                if match:
                    headings.append({
                        'line_num': i,
                        'level': self._get_heading_level(match.group(1) if match.groups() else line),
                        'number': match.group(1) if match.groups() else '',
                        'title': match.group(2) if len(match.groups()) > 1 else line,
                        'full_text': line
                    })
                    break

        return headings

    def _get_heading_level(self, number: str) -> int:
        """‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠"""
        if '‡∏ö‡∏ó‡∏ó‡∏µ‡πà' in number:
            return 1
        if re.match(r'^\d+$', number):  # ‡πÄ‡∏•‡∏Ç‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
            return 2
        if re.match(r'^\d+\.\d+$', number):  # 1.1
            return 3
        if re.match(r'^\d+\.\d+\.\d+$', number):  # 1.1.1
            return 4
        if re.match(r'^[‡∏Å-‡∏Æ]\)$', number):  # ‡∏Å)
            return 5
        return 6

    def split_by_structure(self, text: str) -> List[Dict[str, str]]:
        """
        ‡πÅ‡∏¢‡∏Å text ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠, ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢)
        """
        headings = self.detect_headings(text)
        chunks = []

        if not headings:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ sentence-based chunking
            return self.split_by_sentences(text)

        lines = text.split('\n')

        for i, heading in enumerate(headings):
            start_line = heading['line_num']
            end_line = headings[i + 1]['line_num'] if i + 1 < len(headings) else len(lines)

            # Extract content under this heading
            section_lines = lines[start_line:end_line]
            section_text = '\n'.join(section_lines).strip()

            if section_text:
                # ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÅ‡∏¢‡∏Å‡∏¢‡πà‡∏≠‡∏¢‡∏ï‡πà‡∏≠
                if len(section_text) > self.chunk_size * 1.5:
                    sub_chunks = self.split_large_section(section_text, heading)
                    chunks.extend(sub_chunks)
                else:
                    chunks.append({
                        'text': section_text,
                        'metadata': {
                            'heading': heading['full_text'],
                            'level': heading['level'],
                            'type': 'section'
                        }
                    })

        return chunks

    def split_large_section(self, text: str, heading: Dict) -> List[Dict[str, str]]:
        """
        ‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å ‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏Å‡∏©‡∏≤ context
        """
        # ‡πÉ‡∏ä‡πâ sentence tokenization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        sentences = sent_tokenize(text, engine='crfcut')

        chunks = []
        current_chunk = ""
        current_sentences = []

        for sentence in sentences:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
            if len(current_chunk) + len(sentence) > self.chunk_size and current_chunk:
                chunks.append({
                    'text': current_chunk.strip(),
                    'metadata': {
                        'heading': heading['full_text'],
                        'level': heading['level'],
                        'type': 'subsection',
                        'sentences': len(current_sentences)
                    }
                })

                # ‡πÄ‡∏£‡∏¥‡πà‡∏° chunk ‡πÉ‡∏´‡∏°‡πà ‡∏û‡∏£‡πâ‡∏≠‡∏° overlap
                if self.chunk_overlap > 0 and current_sentences:
                    # ‡πÄ‡∏≠‡∏≤‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏°‡∏≤‡∏ó‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
                    overlap_text = ' '.join(current_sentences[-2:]) if len(current_sentences) > 1 else current_sentences[-1]
                    current_chunk = overlap_text + ' ' + sentence
                    current_sentences = current_sentences[-2:] + [sentence]
                else:
                    current_chunk = sentence
                    current_sentences = [sentence]
            else:
                current_chunk += ' ' + sentence if current_chunk else sentence
                current_sentences.append(sentence)

        # ‡πÄ‡∏û‡∏¥‡πà‡∏° chunk ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'metadata': {
                    'heading': heading['full_text'],
                    'level': heading['level'],
                    'type': 'subsection',
                    'sentences': len(current_sentences)
                }
            })

        return chunks

    def split_by_sentences(self, text: str) -> List[Dict[str, str]]:
        """
        ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)
        """
        sentences = sent_tokenize(text, engine='crfcut')

        chunks = []
        current_chunk = ""
        sentence_count = 0

        for sentence in sentences:
            if len(current_chunk) + len(sentence) > self.chunk_size and current_chunk:
                chunks.append({
                    'text': current_chunk.strip(),
                    'metadata': {
                        'type': 'sentences',
                        'sentence_count': sentence_count
                    }
                })

                # Overlap
                words = current_chunk.split()
                if len(words) > 20:
                    overlap = ' '.join(words[-20:])
                    current_chunk = overlap + ' ' + sentence
                else:
                    current_chunk = sentence

                sentence_count = 1
            else:
                current_chunk += ' ' + sentence if current_chunk else sentence
                sentence_count += 1

        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'metadata': {
                    'type': 'sentences',
                    'sentence_count': sentence_count
                }
            })

        return chunks

    def chunk_text(self, text: str, source: str = "") -> List[Dict[str, str]]:
        """
        Main method: ‡πÅ‡∏¢‡∏Å text ‡∏î‡πâ‡∏ß‡∏¢ semantic chunking
        """
        print(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£...")

        # ‡∏•‡∏≠‡∏á‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô
        chunks = self.split_by_structure(text)

        print(f"‚úÖ ‡πÅ‡∏¢‡∏Å‡πÑ‡∏î‡πâ {len(chunks)} chunks ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")

        # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata
        for i, chunk in enumerate(chunks):
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á unique ID ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ source filename + index
            source_prefix = source.replace('.pdf', '').replace(' ', '_').replace('‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà', 'doc')
            chunk['id'] = f"{source_prefix}_chunk_{i}"
            chunk['source'] = source
            if 'metadata' not in chunk:
                chunk['metadata'] = {}
            chunk['metadata']['chunk_index'] = i
            chunk['metadata']['total_chunks'] = len(chunks)

        return chunks

    def print_chunk_analysis(self, chunks: List[Dict[str, str]]):
        """‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå chunks"""
        print("\nüìä ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Chunks:")
        print("=" * 80)

        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        total_chunks = len(chunks)
        avg_length = sum(len(c['text']) for c in chunks) / total_chunks if total_chunks > 0 else 0
        min_length = min(len(c['text']) for c in chunks) if chunks else 0
        max_length = max(len(c['text']) for c in chunks) if chunks else 0

        print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks: {total_chunks}")
        print(f"‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_length:.0f} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
        print(f"‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏™‡∏∏‡∏î: {min_length} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
        print(f"‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏™‡∏∏‡∏î: {max_length} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")

        # ‡∏ô‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
        types = {}
        for chunk in chunks:
            chunk_type = chunk.get('metadata', {}).get('type', 'unknown')
            types[chunk_type] = types.get(chunk_type, 0) + 1

        print(f"\n‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó chunks:")
        for chunk_type, count in types.items():
            print(f"  - {chunk_type}: {count}")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        print(f"\nüìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á chunks (3 ‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å):")
        print("=" * 80)
        for i, chunk in enumerate(chunks[:3], 1):
            heading = chunk.get('metadata', {}).get('heading', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠')
            text_preview = chunk['text'][:150].replace('\n', ' ')
            print(f"\n{i}. ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {heading}")
            print(f"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß: {len(chunk['text'])} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
            print(f"   ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {text_preview}...")


# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö
def test_semantic_chunker():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö semantic chunker"""
    test_text = """
    ‡∏ö‡∏ó‡∏ó‡∏µ‡πà 1 ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

    1.1 ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
    ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤

    1.2 ‡∏ä‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤
    ‡∏Å) ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï (‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤)
    ‡∏Ç) ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©: Bachelor of Engineering (Civil Engineering and Education)

    1.3 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö
    ‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏° ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡πÄ‡∏´‡∏ô‡∏∑‡∏≠
    """

    chunker = SemanticChunker(chunk_size=200, chunk_overlap=50)
    chunks = chunker.chunk_text(test_text, source="test.pdf")
    chunker.print_chunk_analysis(chunks)


if __name__ == "__main__":
    test_semantic_chunker()
