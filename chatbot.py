import json
import os
from typing import List, Dict, Optional, Tuple
import google.generativeai as genai
from dotenv import load_dotenv
import chromadb
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from pythainlp.tokenize import word_tokenize

class HybridKnowledgeBase:
    """Class to handle hybrid retrieval (BM25 + Vector Search)"""

    def __init__(self, persist_directory: str = "./chroma_db", collection_name: str = "chatbot_knowledge"):
        self.persist_directory = persist_directory
        self.collection_name = collection_name

        # Initialize the embedding model
        print("ü§ñ Loading embedding model...")
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

        # Initialize ChromaDB client
        print(f"üíæ Connecting to ChromaDB at: {persist_directory}")
        self.client = chromadb.PersistentClient(path=persist_directory)

        # Get the collection
        try:
            self.collection = self.client.get_collection(name=collection_name)
            print(f"‚úÖ Connected to collection: {collection_name}")
            print(f"üìö Collection contains {self.collection.count()} documents")
        except Exception as e:
            print(f"‚ùå Error loading collection: {e}")
            raise

        # Load all documents for BM25
        print("üìù Loading documents for BM25 indexing...")
        self._load_documents_for_bm25()

    def _load_documents_for_bm25(self):
        """Load all documents and create BM25 index"""
        # Get all documents from ChromaDB
        all_docs = self.collection.get()
        self.documents = all_docs['documents']
        self.doc_ids = all_docs['ids']
        
        # Tokenize documents for BM25 using Thai word tokenizer
        print("üî§ Tokenizing documents for BM25...")
        self.tokenized_docs = [word_tokenize(doc, engine='newmm') for doc in self.documents]
        
        # Create BM25 index
        print("üîç Building BM25 index...")
        self.bm25 = BM25Okapi(self.tokenized_docs)
        print(f"‚úÖ BM25 index ready with {len(self.documents)} documents")

    def _bm25_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """
        Perform BM25 keyword search
        
        Args:
            query: User's question
            top_k: Number of top results to return
            
        Returns:
            List of (doc_index, score) tuples
        """
        # Tokenize query using Thai tokenizer
        tokenized_query = word_tokenize(query, engine='newmm')
        
        # Get BM25 scores
        bm25_scores = self.bm25.get_scores(tokenized_query)
        
        # Get top-k results with indices
        top_indices = bm25_scores.argsort()[-top_k:][::-1]
        results = [(idx, bm25_scores[idx]) for idx in top_indices if bm25_scores[idx] > 0]
        
        return results
    
    def _vector_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """
        Perform vector similarity search
        
        Args:
            query: User's question
            top_k: Number of top results to return
            
        Returns:
            List of (doc_index, similarity_score) tuples
        """
        # Generate embedding for the query
        query_embedding = self.model.encode([query])

        # Search in vector database
        results = self.collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=top_k
        )

        # Convert to (index, score) format
        # ChromaDB returns distance (lower is better), convert to similarity (higher is better)
        vector_results = []
        if results['documents'][0]:
            for doc_text, distance in zip(results['documents'][0], results['distances'][0]):
                # Find document index
                try:
                    doc_idx = self.documents.index(doc_text)
                    # Convert distance to similarity score (1 - normalized_distance)
                    similarity = 1 / (1 + distance)  # Higher score = more similar
                    vector_results.append((doc_idx, similarity))
                except ValueError:
                    continue
        
        return vector_results
    
    def search_knowledge(self, query: str, n_results: int = 5, 
                        bm25_weight: float = 0.4, vector_weight: float = 0.6) -> List[Dict[str, str]]:
        """
        Hybrid search combining BM25 and vector similarity
        
        Args:
            query: User's question
            n_results: Number of top results to return
            bm25_weight: Weight for BM25 scores (default 0.4)
            vector_weight: Weight for vector scores (default 0.6)
            
        Returns:
            List of relevant knowledge items with text and relevance score
        """
        # Perform both searches
        bm25_results = self._bm25_search(query, top_k=15)
        vector_results = self._vector_search(query, top_k=15)
        
        # Normalize scores for both methods
        def normalize_scores(results: List[Tuple[int, float]]) -> Dict[int, float]:
            if not results:
                return {}
            scores = [score for _, score in results]
            min_score = min(scores)
            max_score = max(scores)
            
            if max_score == min_score:
                return {idx: 1.0 for idx, _ in results}
            
            return {
                idx: (score - min_score) / (max_score - min_score)
                for idx, score in results
            }
        
        bm25_normalized = normalize_scores(bm25_results)
        vector_normalized = normalize_scores(vector_results)
        
        # Combine scores using weighted sum
        combined_scores = {}
        all_indices = set(bm25_normalized.keys()) | set(vector_normalized.keys())
        
        for idx in all_indices:
            bm25_score = bm25_normalized.get(idx, 0.0)
            vector_score = vector_normalized.get(idx, 0.0)
            
            # Weighted combination
            combined_scores[idx] = (bm25_weight * bm25_score) + (vector_weight * vector_score)
        
        # Sort by combined score and get top results
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:n_results]
        
        # Format results
        relevant_knowledge = []
        for rank, (idx, score) in enumerate(sorted_results, 1):
            relevant_knowledge.append({
                'text': self.documents[idx],
                'score': score,
                'rank': rank,
                'bm25_score': bm25_normalized.get(idx, 0.0),
                'vector_score': vector_normalized.get(idx, 0.0)
            })
        
        return relevant_knowledge

    def get_context_string(self, relevant_items: List[Dict[str, str]]) -> str:
        """Convert relevant knowledge items to context string"""
        if not relevant_items:
            return ""

        context = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ:\n\n"
        for item in relevant_items:
            context += f"{item['rank']}. {item['text']}\n\n"

        return context


class GeminiChatbot:
    """Main chatbot class using Gemini API"""

    def __init__(self, api_key: str, knowledge_base: HybridKnowledgeBase):
        self.api_key = api_key
        self.knowledge_base = knowledge_base
        self.setup_gemini()
        self.conversation_history = []
    
    def setup_gemini(self):
        """Initialize Gemini API"""
        try:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-2.5-flash')
            print("‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Gemini API ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Gemini API: {e}")
            raise
    
    def expand_query(self, query: str) -> str:
        """
        Expand user query to improve search results for natural language questions
        """
        # Common conversational patterns in Thai
        query_lower = query.lower()

        # Normalize casual questions
        if any(word in query_lower for word in ['‡∏≠‡∏¢‡∏≤‡∏Å', '‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£', '‡∏™‡∏ô‡πÉ‡∏à']):
            # User expressing interest
            if '‡∏£‡∏π‡πâ' in query_lower or '‡∏ó‡∏£‡∏≤‡∏ö' in query_lower:
                query += " ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"

        # Normalize question words
        if '‡∏ó‡∏≥‡πÑ‡∏á' in query_lower or '‡∏¢‡∏±‡∏á‡πÑ‡∏á' in query_lower:
            query += " ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô"

        if '‡∏°‡∏µ' in query_lower and '‡πÑ‡∏´‡∏°' in query_lower:
            query += " ‡∏°‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"

        # Common topics expansion
        if '‡∏™‡∏°‡∏±‡∏Ñ‡∏£' in query_lower:
            query += " ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"

        if '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô' in query_lower:
            query += " ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤"

        if '‡∏Ñ‡πà‡∏≤' in query_lower and ('‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢' in query_lower or '‡πÄ‡∏ó‡∏≠‡∏°' in query_lower or '‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô' in query_lower):
            query += " ‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏° ‡∏Ñ‡πà‡∏≤‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢"

        return query

    def create_prompt(self, user_question: str) -> str:
        """Create a comprehensive prompt with context"""
        # Expand query for better search
        expanded_query = self.expand_query(user_question)

        # Search for relevant knowledge with more results
        relevant_knowledge = self.knowledge_base.search_knowledge(expanded_query, n_results=8)
        context = self.knowledge_base.get_context_string(relevant_knowledge)

        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "‡∏ô‡πâ‡∏≠‡∏á‡∏ö‡∏≠‡∏ó" ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤
‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏° ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡πÄ‡∏´‡∏ô‡∏∑‡∏≠ (‡∏°‡∏à‡∏û.)

**‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:**
- ‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏û‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ô‡πâ‡∏≠‡∏á ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
- ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞" ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
- ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡πÑ‡∏°‡πà‡∏¢‡∏∑‡∏î‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
- ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏ñ‡∏≤‡∏°‡∏Å‡∏•‡∏±‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û

**‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:**
1. ‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠ (‡∏ï‡∏£‡∏á‡πÜ ‡πÑ‡∏°‡πà‡∏≠‡πâ‡∏≠‡∏°‡∏Ñ‡πâ‡∏≠‡∏°)
2. ‡πÉ‡∏´‡πâ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
3. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ö‡∏≠‡∏Å‡∏ï‡∏£‡∏á‡πÜ ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:**
{context if context else "(‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ)"}

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ:** {user_question}

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
- ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö" ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠:
  * ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå: admission.kmutnb.ac.th
  * ‡πÇ‡∏ó‡∏£: 02-555-2000 (‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ì‡∏∞)
  * Facebook: ‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏° ‡∏°‡∏à‡∏û.

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ:**
‚ùå "‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏°‡∏µ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 5 ‡∏õ‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö"
‚úÖ "‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô 5 ‡∏õ‡∏µ‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏à‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ‡∏ß‡∏∏‡∏í‡∏¥‡∏Ñ‡∏£‡∏π‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡πÄ‡∏•‡∏¢"

**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:**"""

        return prompt
    
    def chat(self, user_input: str) -> str:
        """Process user input and return chatbot response"""
        try:
            # Create prompt with knowledge base context
            prompt = self.create_prompt(user_input)
            
            # Generate response using Gemini
            response = self.model.generate_content(prompt)
            
            if response.text:
                # Store conversation history
                self.conversation_history.append({
                    "user": user_input,
                    "bot": response.text
                })
                return response.text
            else:
                return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
                
        except Exception as e:
            print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """Get conversation history"""
        return self.conversation_history
    
    def clear_history(self):
        """Clear conversation history"""
        self.conversation_history = []


def main():
    """Main function to run the chatbot"""

    # Load environment variables
    load_dotenv()

    # Get API key
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö API Key ‡πÉ‡∏ô environment variables")
        return

    try:
        # Initialize vector knowledge base and chatbot
        print("=" * 80)
        print("üîß Initializing chatbot with hybrid retrieval system...")
        print("=" * 80)
        kb = HybridKnowledgeBase(persist_directory="./chroma_db", collection_name="chatbot_knowledge")
        chatbot = GeminiChatbot(api_key, kb)
        
        print("=" * 80)
        print("ü§ñ ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°")
        print("    ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡πÄ‡∏´‡∏ô‡∏∑‡∏≠")
        print("=" * 80)
        print("üìö ‡∏£‡∏∞‡∏ö‡∏ö: Hybrid Retrieval (BM25 + Vector Search)")
        print("üîç ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ: BM25 (keyword) + ChromaDB (semantic) + PyThaiNLP")
        print("-" * 80)
        print("üìù ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ")
        print("üí° ‡∏û‡∏¥‡∏°‡∏û‡πå 'quit', 'exit', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏≠‡∏≠‡∏Å' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏¥‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°")
        print("üîÑ ‡∏û‡∏¥‡∏°‡∏û‡πå 'clear' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
        print("üìú ‡∏û‡∏¥‡∏°‡∏û‡πå 'history' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
        print("-" * 80)
        
        while True:
            # Get user input
            user_input = input("\nüôã ‡∏Ñ‡∏∏‡∏ì: ").strip()
            
            # Check for exit commands
            if user_input.lower() in ['quit', 'exit', '‡∏≠‡∏≠‡∏Å', '']:
                print("\nüëã ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£! ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞")
                break
            
            # Check for clear command
            if user_input.lower() in ['clear', '‡∏•‡πâ‡∏≤‡∏á']:
                chatbot.clear_history()
                print("\nüßπ ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
                continue
            
            # Check for history command
            if user_input.lower() in ['history', '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥']:
                history = chatbot.get_conversation_history()
                if history:
                    print("\nüìú ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:")
                    print("-" * 40)
                    for i, conv in enumerate(history, 1):
                        print(f"[{i}] ‡∏Ñ‡∏∏‡∏ì: {conv['user']}")
                        print(f"[{i}] ‡∏ö‡∏≠‡∏ó: {conv['bot'][:100]}...")
                        print("-" * 40)
                else:
                    print("\nüì≠ ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
                continue
            
            # Process the question
            print("\nü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î...")
            response = chatbot.chat(user_input)
            print(f"\nü§ñ ‡∏ö‡∏≠‡∏ó: {response}")
            
    except KeyboardInterrupt:
        print("\n\nüëã ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£! ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞")
    except Exception as e:
        print(f"\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")


if __name__ == "__main__":
    main()
