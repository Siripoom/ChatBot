import json
import os
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
from dotenv import load_dotenv
import chromadb
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
from pythainlp.tokenize import word_tokenize

class HybridKnowledgeBase:
    """Class to handle hybrid retrieval (BM25 + Vector Search)"""

    def __init__(self, persist_directory: str = "./chroma_db", collection_name: str = "chatbot_knowledge",
                 use_reranker: bool = True):
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.use_reranker = use_reranker

        # Initialize the embedding model
        print("ü§ñ Loading embedding model...")
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

        # Initialize Cross-Encoder for re-ranking
        if self.use_reranker:
            print("üéØ Loading Cross-Encoder for re-ranking...")
            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            print("‚úÖ Re-ranker loaded successfully")

        # Initialize ChromaDB client
        print(f"üíæ Connecting to ChromaDB at: {persist_directory}")
        self.client = chromadb.PersistentClient(path=persist_directory)

        # Get the collection
        try:
            self.collection = self.client.get_collection(name=collection_name)
            print(f"‚úÖ Connected to collection: {collection_name}")
            print(f"üìö Collection contains {self.collection.count()} documents")
        except Exception as e:
            print(f"‚ùå Error loading collection: {e}")
            raise

        # Load all documents for BM25
        print("üìù Loading documents for BM25 indexing...")
        self._load_documents_for_bm25()

    def _load_documents_for_bm25(self):
        """Load all documents and create BM25 index"""
        # Get all documents from ChromaDB
        all_docs = self.collection.get(include=['documents', 'metadatas'])
        self.documents = all_docs['documents']
        self.doc_ids = all_docs['ids']
        self.metadatas = all_docs['metadatas']

        # Tokenize documents for BM25 using Thai word tokenizer
        print("üî§ Tokenizing documents for BM25...")
        self.tokenized_docs = [word_tokenize(doc, engine='newmm') for doc in self.documents]

        # Create BM25 index
        print("üîç Building BM25 index...")
        self.bm25 = BM25Okapi(self.tokenized_docs)
        print(f"‚úÖ BM25 index ready with {len(self.documents)} documents")

    def _bm25_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """
        Perform BM25 keyword search

        Args:
            query: User's question
            top_k: Number of top results to return

        Returns:
            List of (doc_index, score) tuples
        """
        # Tokenize query using Thai tokenizer
        tokenized_query = word_tokenize(query, engine='newmm')

        # Get BM25 scores
        bm25_scores = self.bm25.get_scores(tokenized_query)

        # Get top-k results with indices
        top_indices = bm25_scores.argsort()[-top_k:][::-1]
        results = [(idx, bm25_scores[idx]) for idx in top_indices if bm25_scores[idx] > 0]

        return results

    def _vector_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """
        Perform vector similarity search

        Args:
            query: User's question
            top_k: Number of top results to return

        Returns:
            List of (doc_index, similarity_score) tuples
        """
        # Generate embedding for the query
        query_embedding = self.model.encode([query])

        # Search in vector database
        results = self.collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=top_k
        )

        # Convert to (index, score) format
        # ChromaDB returns distance (lower is better), convert to similarity (higher is better)
        vector_results = []
        if results['documents'][0]:
            for doc_text, distance in zip(results['documents'][0], results['distances'][0]):
                # Find document index
                try:
                    doc_idx = self.documents.index(doc_text)
                    # Convert distance to similarity score (1 - normalized_distance)
                    similarity = 1 / (1 + distance)  # Higher score = more similar
                    vector_results.append((doc_idx, similarity))
                except ValueError:
                    continue

        return vector_results

    def _rerank(self, query: str, documents: List[Dict[str, any]], top_k: int = 5) -> List[Dict[str, any]]:
        """
        Re-rank documents using Cross-Encoder

        Args:
            query: User's question
            documents: List of documents with their scores from hybrid search
            top_k: Number of top results to return after re-ranking

        Returns:
            Re-ranked list of documents
        """
        if not self.use_reranker or not documents:
            return documents[:top_k]

        # Prepare pairs of (query, document) for cross-encoder
        pairs = [(query, doc['text']) for doc in documents]

        # Get re-ranking scores
        rerank_scores = self.reranker.predict(pairs)

        # Add rerank scores to documents
        for doc, score in zip(documents, rerank_scores):
            doc['rerank_score'] = float(score)

        # Sort by rerank score and return top-k
        reranked = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)[:top_k]

        # Update ranks after re-ranking
        for rank, doc in enumerate(reranked, 1):
            doc['rank'] = rank

        return reranked

    def search_knowledge(self, query: str, n_results: int = 5,
                        bm25_weight: float = 0.4, vector_weight: float = 0.6) -> List[Dict[str, str]]:
        """
        Hybrid search combining BM25 and vector similarity

        Args:
            query: User's question
            n_results: Number of top results to return
            bm25_weight: Weight for BM25 scores (default 0.4)
            vector_weight: Weight for vector scores (default 0.6)

        Returns:
            List of relevant knowledge items with text and relevance score
        """
        # Perform both searches
        bm25_results = self._bm25_search(query, top_k=15)
        vector_results = self._vector_search(query, top_k=15)

        # Normalize scores for both methods
        def normalize_scores(results: List[Tuple[int, float]]) -> Dict[int, float]:
            if not results:
                return {}
            scores = [score for _, score in results]
            min_score = min(scores)
            max_score = max(scores)

            if max_score == min_score:
                return {idx: 1.0 for idx, _ in results}

            return {
                idx: (score - min_score) / (max_score - min_score)
                for idx, score in results
            }

        bm25_normalized = normalize_scores(bm25_results)
        vector_normalized = normalize_scores(vector_results)

        # Combine scores using weighted sum
        combined_scores = {}
        all_indices = set(bm25_normalized.keys()) | set(vector_normalized.keys())

        for idx in all_indices:
            bm25_score = bm25_normalized.get(idx, 0.0)
            vector_score = vector_normalized.get(idx, 0.0)

            # Weighted combination
            combined_scores[idx] = (bm25_weight * bm25_score) + (vector_weight * vector_score)

        # Sort by combined score and get more results for re-ranking
        top_k_for_rerank = min(n_results * 3, 15)  # Get 3x results for re-ranking
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k_for_rerank]

        # Format results
        relevant_knowledge = []
        for rank, (idx, score) in enumerate(sorted_results, 1):
            relevant_knowledge.append({
                'text': self.documents[idx],
                'score': score,
                'rank': rank,
                'bm25_score': bm25_normalized.get(idx, 0.0),
                'vector_score': vector_normalized.get(idx, 0.0),
                'metadata': self.metadatas[idx] if idx < len(self.metadatas) else {}
            })

        # Apply re-ranking if enabled
        if self.use_reranker and relevant_knowledge:
            relevant_knowledge = self._rerank(query, relevant_knowledge, top_k=n_results)
        else:
            relevant_knowledge = relevant_knowledge[:n_results]

        return relevant_knowledge

    def get_context_string(self, relevant_items: List[Dict[str, str]]) -> str:
        """Convert relevant knowledge items to context string"""
        if not relevant_items:
            return ""

        context = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ:\n\n"
        for item in relevant_items:
            context += f"{item['rank']}. {item['text']}\n\n"

        return context


class TyphoonChatbot:
    """Main chatbot class using Typhoon API"""

    def __init__(self, api_key: str, knowledge_base: HybridKnowledgeBase,
                 use_compression: bool = True):
        self.api_key = api_key
        self.knowledge_base = knowledge_base
        self.use_compression = use_compression
        self.setup_typhoon()
        self.conversation_history = []

    def setup_typhoon(self):
        """Initialize Typhoon API"""
        try:
            self.client = OpenAI(
                api_key=self.api_key,
                base_url="https://api.opentyphoon.ai/v1"
            )
            print("‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Typhoon API ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Typhoon API: {e}")
            raise

    def expand_query(self, query: str) -> str:
        """
        Expand user query to improve search results for natural language questions (TH focus).
        ‡πÅ‡∏Å‡πâ/‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ CED/TCT/TM
        """
        q = query.strip()
        ql = q.lower()

        # üéØ ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢: ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÅ‡∏Ñ‡πà "‡∏Å‡∏µ‡πà‡∏õ‡∏µ" ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏≠‡∏∑‡πà‡∏ô ‡πÉ‡∏´‡πâ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å
        is_simple_duration_question = (
            any(w in ql for w in ['‡∏Å‡∏µ‡πà‡∏õ‡∏µ', '‡∏ô‡∏≤‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà', '‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤']) and
            not any(w in ql for w in ['‡∏ß‡∏¥‡∏ä‡∏≤', '‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤', '‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡πÅ‡∏ú‡∏ô', '‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå', '‡∏Ñ‡πà‡∏≤'])
        )

        if is_simple_duration_question:
            # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            q += " ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ ‡∏õ‡∏µ ‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏•‡∏≠‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤"
            return q  # ‚ö†Ô∏è Return ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏¢‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°

        # 1) ‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ/‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏π‡∏î
        if any(w in ql for w in ['‡∏≠‡∏¢‡∏≤‡∏Å', '‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£', '‡∏™‡∏ô‡πÉ‡∏à']):
            if '‡∏£‡∏π‡πâ' in ql or '‡∏ó‡∏£‡∏≤‡∏ö' in ql:
                q += " ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"

        # 2) ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏õ‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡∏¥‡∏î/‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥
        if any(w in ql for w in ['‡∏ó‡∏≥‡πÑ‡∏á', '‡∏¢‡∏±‡∏á‡πÑ‡∏á']):
            q += " ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô"
        if '‡∏°‡∏µ' in ql and '‡πÑ‡∏´‡∏°' in ql:
            q += " ‡∏°‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"
        if any(w in ql for w in ['‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£', '‡∏Ñ‡∏∑‡∏≠ ‡∏≠‡∏∞‡πÑ‡∏£', '‡∏Ñ‡∏∑‡∏≠?', '‡∏Ñ‡∏∑‡∏≠ ']):
            q += " ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡∏Ñ‡∏≥‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏ô‡∏¥‡∏¢‡∏≤‡∏°"
        if any(w in ql for w in ['‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô', '‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö', '‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö']):
            q += " ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö ‡πÄ‡∏Å‡∏ì‡∏ë‡πå"

        # 3) ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£/‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥/‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£/‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏≠‡∏ô
        if any(w in ql for w in ['‡∏™‡∏°‡∏±‡∏Ñ‡∏£', '‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤', '‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥', '‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏≠‡∏ô', '‡πÇ‡∏≠‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï']):
            q += " ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏≠‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"

        # 4) ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£/‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï/‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤
        if any(w in ql for w in ['‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï', '‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á', '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', '‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤']):
            q += " ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï ‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡πÅ‡∏Å‡∏ô ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏™‡∏£‡∏µ ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô"

        # 5) ‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤/‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤/‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô/‡∏§‡∏î‡∏π‡∏£‡πâ‡∏≠‡∏ô
        # ‚ö†Ô∏è ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÅ‡∏Ñ‡πà "‡∏Å‡∏µ‡πà‡∏õ‡∏µ" ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô
        if any(w in ql for w in ['‡∏Å‡∏µ‡πà‡∏õ‡∏µ', '‡∏ô‡∏≤‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà', '‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤']):
            # ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÅ‡∏Ñ‡πà‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ ‡∏Ç‡∏¢‡∏≤‡∏¢‡πÅ‡∏Ñ‡πà‡∏ô‡∏¥‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            if any(w in ql for w in ['‡∏Å‡∏µ‡πà‡∏õ‡∏µ', '‡∏ô‡∏≤‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà', '‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤']) and not any(w in ql for w in ['‡∏ß‡∏¥‡∏ä‡∏≤', '‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤', '‡πÅ‡∏ú‡∏ô', '‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á']):
                q += " ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏•‡∏≠‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£"
            else:
                q += " ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏ß‡∏¥‡∏†‡∏≤‡∏Ñ ‡∏õ‡∏è‡∏¥‡∏ó‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏†‡∏≤‡∏Ñ‡∏§‡∏î‡∏π‡∏£‡πâ‡∏≠‡∏ô ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏≤‡∏Ñ"
        elif any(w in ql for w in ['‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ó‡∏≠‡∏°', '‡∏§‡∏î‡∏π‡∏£‡πâ‡∏≠‡∏ô']):
            q += " ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏ß‡∏¥‡∏†‡∏≤‡∏Ñ ‡∏õ‡∏è‡∏¥‡∏ó‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏†‡∏≤‡∏Ñ‡∏§‡∏î‡∏π‡∏£‡πâ‡∏≠‡∏ô ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏≤‡∏Ñ"

        # 6) ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤/‡πÅ‡∏ú‡∏ô‡∏£‡∏≤‡∏¢‡∏õ‡∏µ/‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
        if any(w in ql for w in ['‡∏ß‡∏¥‡∏ä‡∏≤', '‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤', '‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤', '‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡∏õ‡∏µ‡∏ó‡∏µ‡πà']):
            q += " ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏£‡∏≤‡∏¢‡∏õ‡∏µ ‡∏™‡∏°‡∏£‡∏£‡∏ñ‡∏ô‡∏∞‡∏ú‡∏π‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤"

        # 7) ‡∏ù‡∏∂‡∏Å‡∏á‡∏≤‡∏ô/‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô/‡∏†‡∏≤‡∏Ñ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥/‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
        if any(w in ql for w in ['‡∏ù‡∏∂‡∏Å‡∏á‡∏≤‡∏ô', '‡∏™‡∏´‡∏Å‡∏¥‡∏à', '‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô', '‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô', '‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á']):
            q += " ‡∏ù‡∏∂‡∏Å‡∏á‡∏≤‡∏ô S/U ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô ‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏° ‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏ù‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡πà‡∏≠‡∏ô‡∏ù‡∏∂‡∏Å"

        # 8) ‡∏†‡∏≤‡∏©‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô
        if any(w in ql for w in ['‡∏†‡∏≤‡∏©‡∏≤', 'english', '‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', '‡∏™‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤', 'bilingual']):
            q += " ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£/‡∏ï‡∏≥‡∏£‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©"

        # 9) ‡∏≠‡∏≤‡∏ä‡∏µ‡∏û/‡∏à‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á/‡∏ß‡∏∏‡∏í‡∏¥/‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û
        if any(w in ql for w in ['‡∏à‡∏ö‡πÅ‡∏•‡πâ‡∏ß', '‡∏à‡∏ö‡πÑ‡∏õ', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤', '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏∞‡πÑ‡∏£', '‡∏≠‡∏≤‡∏ä‡∏µ‡∏û', '‡∏ß‡∏∏‡∏í‡∏¥']):
            q += " ‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤ ‡∏ß‡∏∏‡∏í‡∏¥‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏î‡πâ ‡∏ô‡∏±‡∏Å‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏∞‡∏ö‡∏ö ‡∏Ñ‡∏£‡∏π‡∏ä‡πà‡∏≤‡∏á ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏° ‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÉ‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"

        # 10) ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô/‡∏Ñ‡∏ì‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå/‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô/‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠
        if any(w in ql for w in ['‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå', '‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö', '‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ô', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠', '‡∏û‡∏±‡∏ô‡∏ò‡∏°‡∏¥‡∏ï‡∏£']):
            q += " ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡∏Ñ‡∏∏‡∏ì‡∏ß‡∏∏‡∏í‡∏¥‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô/‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó"

        # 11) ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û/‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≥‡∏Å‡∏±‡∏ö (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏∏‡∏£‡∏∏‡∏™‡∏†‡∏≤/‡∏™‡∏†‡∏≤‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£)
        if any(w in ql for w in ['‡∏Ñ‡∏∏‡∏£‡∏∏‡∏™‡∏†‡∏≤', '‡∏Ñ‡∏£‡∏π‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°', '‡∏™‡∏†‡∏≤‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£', '‡∏ß‡∏®.']):
            q += " ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û‡∏Ñ‡∏£‡∏π ‡∏°‡∏Ñ‡∏≠ ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏†‡∏≤‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û‡πÅ‡∏•‡∏∞‡∏à‡∏£‡∏£‡∏¢‡∏≤‡∏ö‡∏£‡∏£‡∏ì"

        # 12) ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≤‡∏Ç‡∏≤ (‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ / ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏®‡∏∂‡∏Å‡∏©‡∏≤)
        if any(w in ql for w in ['‡πÇ‡∏¢‡∏ò‡∏≤', '‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤', 'civil']):
            q += " ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ 5 ‡∏õ‡∏µ ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ STEM ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ ‡∏á‡∏≤‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏†‡∏≤‡∏Ñ‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°"

        # ‚ö†Ô∏è ‡∏û‡∏¥‡πÄ‡∏®‡∏©: ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á "‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏≠‡∏ô" + "‡∏Å‡∏µ‡πà‡∏õ‡∏µ" ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ß‡∏¥‡∏ä‡∏≤ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Ñ‡πà‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤
        if any(w in ql for w in ['‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏≠‡∏ô', '‡∏õ‡∏ß‡∏™']) and any(w in ql for w in ['‡∏Å‡∏µ‡πà‡∏õ‡∏µ', '‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', '‡∏ô‡∏≤‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà']):
            q += " ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏≠‡∏ô ‡∏õ‡∏ß‡∏™ 3 ‡∏õ‡∏µ 4 ‡∏õ‡∏µ ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏•‡∏≠‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£"
        elif any(w in ql for w in ['‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏®‡∏∂‡∏Å‡∏©‡∏≤', '‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå', 'ced', 'tct', 'computer']):
            q += " ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏£‡∏≤‡∏¢‡∏õ‡∏µ IoT ‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏•‡∏∞‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠"

        # 13) ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏Ñ‡πà‡∏≤' ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞)
        if '‡∏Ñ‡πà‡∏≤' in ql and any(w in ql for w in ['‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢', '‡πÄ‡∏ó‡∏≠‡∏°', '‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô', '‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°']):
            q += " ‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏° ‡∏Ñ‡πà‡∏≤‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£"

        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤
        return q

    def compress_context(self, query: str, contexts: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Compress context by filtering only relevant parts using LLM

        Args:
            query: User's question
            contexts: List of context items with text

        Returns:
            Compressed list of context items
        """
        if not self.use_compression or not contexts:
            return contexts

        compressed_contexts = []

        for ctx in contexts:
            # Create compression prompt
            compression_prompt = f"""‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {ctx['text']}

‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ "YES" ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏´‡∏£‡∏∑‡∏≠ "NO" ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡∏ï‡∏≠‡∏ö YES ‡πÉ‡∏´‡πâ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 2-3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ)

‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
[YES/NO]
[‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (‡∏ñ‡πâ‡∏≤ YES)]"""

            try:
                response = self.client.chat.completions.create(
                    model="typhoon-v2.5-30b-a3b-instruct",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that analyzes text relevance. Answer in Thai."},
                        {"role": "user", "content": compression_prompt}
                    ],
                    max_tokens=256,
                    temperature=0.3
                )
                response_text = response.choices[0].message.content.strip()

                # Check if context is relevant
                if response_text.startswith("YES"):
                    # Extract the compressed text
                    lines = response_text.split('\n', 1)
                    compressed_text = lines[1].strip() if len(lines) > 1 else ctx['text']

                    # Keep the context with compressed text
                    compressed_ctx = ctx.copy()
                    compressed_ctx['text'] = compressed_text
                    compressed_ctx['original_text'] = ctx['text']
                    compressed_ctx['compressed'] = True
                    compressed_contexts.append(compressed_ctx)

            except Exception as e:
                # If compression fails, keep original
                print(f"‚ö†Ô∏è Compression failed for a context: {e}")
                compressed_contexts.append(ctx)

        return compressed_contexts if compressed_contexts else contexts[:2]

    def create_prompt(self, user_question: str) -> str:
        """Create a comprehensive prompt with context"""
        # Expand query for better search
        expanded_query = self.expand_query(user_question)

        # Search for relevant knowledge with more results
        relevant_knowledge = self.knowledge_base.search_knowledge(expanded_query, n_results=10)

        # Apply context compression if enabled (‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß)
        # if self.use_compression:
        #     relevant_knowledge = self.compress_context(user_question, relevant_knowledge)

        context = self.knowledge_base.get_context_string(relevant_knowledge)

        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "‡∏ô‡πâ‡∏≠‡∏á‡∏ö‡∏≠‡∏ó" ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤
‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏° ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡πÄ‡∏´‡∏ô‡∏∑‡∏≠ (‡∏°‡∏à‡∏û.)

**‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (STRICT RAG - ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°):**
1. ‚úÖ ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á" ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
2. ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
3. ‚úÖ ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏°‡∏≤‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
4. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ
5. ‚ùå ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏°‡∏≤‡∏ï‡∏≠‡∏ö
6. ‚úÖ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ï‡∏£‡∏á‡πÜ ‡∏ß‡πà‡∏≤ "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö" ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠

**‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:**
- ‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏û‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ô‡πâ‡∏≠‡∏á
- ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞" ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö
- ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏°‡∏≤‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô):**
{context if context else "(‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ)"}

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ:** {user_question}

**‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏≠‡∏ö:**
- ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 10 ‡∏Ç‡πâ‡∏≠ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
- ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏°‡∏≤‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ï‡∏£‡∏á‡πÜ ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠

**‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö:**
- ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå: admission.kmutnb.ac.th
- ‡πÇ‡∏ó‡∏£: 02-555-2000 (‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ì‡∏∞)
- Facebook: ‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏° ‡∏°‡∏à‡∏û.

**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô):**"""

        return prompt

    def chat(self, user_input: str) -> str:
        """Process user input and return chatbot response"""
        try:
            # Create prompt with knowledge base context
            prompt = self.create_prompt(user_input)

            # Generate response using Typhoon
            response = self.client.chat.completions.create(
                model="typhoon-v2.1-12b-instruct",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant. You must answer only in Thai."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1024,
                temperature=0.6
            )

            if response.choices[0].message.content:
                # Store conversation history
                self.conversation_history.append({
                    "user": user_input,
                    "bot": response.choices[0].message.content
                })
                return response.choices[0].message.content
            else:
                return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"

        except Exception as e:
            print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """Get conversation history"""
        return self.conversation_history

    def clear_history(self):
        """Clear conversation history"""
        self.conversation_history = []


def main():
    """Main function to run the chatbot"""

    # Load environment variables
    load_dotenv()

    # Get API key
    api_key = os.getenv('TYPHOON_API_KEY')
    if not api_key:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö TYPHOON_API_KEY ‡πÉ‡∏ô environment variables")
        return

    try:
        # Initialize vector knowledge base and chatbot
        print("=" * 80)
        print("üîß Initializing chatbot with hybrid retrieval system...")
        print("=" * 80)
        kb = HybridKnowledgeBase(persist_directory="./chroma_db", collection_name="chatbot_knowledge")
        chatbot = TyphoonChatbot(api_key, kb)

        print("=" * 80)
        print("ü§ñ ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏° (Typhoon v2.1)")
        print("    ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡πÄ‡∏´‡∏ô‡∏∑‡∏≠")
        print("=" * 80)
        print("üìö ‡∏£‡∏∞‡∏ö‡∏ö: Hybrid Retrieval (BM25 + Vector Search)")
        print("üîç ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ: BM25 (keyword) + ChromaDB (semantic) + PyThaiNLP")
        print("üå™Ô∏è  AI Model: Typhoon v2.1-12b-instruct")
        print("-" * 80)
        print("üìù ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ")
        print("üí° ‡∏û‡∏¥‡∏°‡∏û‡πå 'quit', 'exit', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏≠‡∏≠‡∏Å' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏¥‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°")
        print("üîÑ ‡∏û‡∏¥‡∏°‡∏û‡πå 'clear' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
        print("üìú ‡∏û‡∏¥‡∏°‡∏û‡πå 'history' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
        print("-" * 80)

        while True:
            # Get user input
            user_input = input("\nüôã ‡∏Ñ‡∏∏‡∏ì: ").strip()

            # Check for exit commands
            if user_input.lower() in ['quit', 'exit', '‡∏≠‡∏≠‡∏Å', '']:
                print("\nüëã ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£! ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞")
                break

            # Check for clear command
            if user_input.lower() in ['clear', '‡∏•‡πâ‡∏≤‡∏á']:
                chatbot.clear_history()
                print("\nüßπ ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
                continue

            # Check for history command
            if user_input.lower() in ['history', '‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥']:
                history = chatbot.get_conversation_history()
                if history:
                    print("\nüìú ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:")
                    print("-" * 40)
                    for i, conv in enumerate(history, 1):
                        print(f"[{i}] ‡∏Ñ‡∏∏‡∏ì: {conv['user']}")
                        print(f"[{i}] ‡∏ö‡∏≠‡∏ó: {conv['bot'][:100]}...")
                        print("-" * 40)
                else:
                    print("\nüì≠ ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
                continue

            # Process the question
            print("\nü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î...")
            response = chatbot.chat(user_input)
            print(f"\nü§ñ ‡∏ö‡∏≠‡∏ó: {response}")

    except KeyboardInterrupt:
        print("\n\nüëã ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£! ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞")
    except Exception as e:
        print(f"\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")


if __name__ == "__main__":
    main()
