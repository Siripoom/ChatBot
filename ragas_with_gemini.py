"""
RAGAS Evaluation with Google Gemini (No OpenAI Required)
=========================================================
This script evaluates the RAG chatbot using RAGAS metrics with Google Gemini
instead of OpenAI for evaluation.

RAGAS Metrics:
- Faithfulness: How factually accurate is the answer based on the context?
- Answer Relevancy: How relevant is the answer to the question?
- Context Precision: How precise are the retrieved contexts?
- Context Recall: How well do contexts cover the ground truth?
"""

import os
import json
from typing import List, Dict
from dotenv import load_dotenv
from datasets import Dataset
import pandas as pd

# Import the chatbot
from chatbot_v04_keywords import HybridKnowledgeBase, TyphoonChatbot

# RAGAS imports
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    )
    # Use Langchain Google Generative AI instead of OpenAI
    from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
except ImportError as e:
    print("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á required libraries:")
    print("   pip install ragas langchain-google-genai")
    print(f"   Error: {e}")
    exit(1)


class RAGASEvaluatorGemini:
    """Class for evaluating RAG chatbot using RAGAS metrics with Google Gemini"""

    def __init__(self, chatbot: TyphoonChatbot, gemini_api_key: str):
        """
        Initialize RAGAS evaluator with Google Gemini

        Args:
            chatbot: TyphoonChatbot instance to evaluate
            gemini_api_key: Google Gemini API key for RAGAS evaluation
        """
        self.chatbot = chatbot
        self.gemini_api_key = gemini_api_key

        # Initialize Gemini LLM and Embeddings for RAGAS
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-3-flash-preview",
            google_api_key=gemini_api_key,
            temperature=0,
            convert_system_message_to_human=True  # Important for compatibility
        )

        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=gemini_api_key
        )

        print("‚úÖ Initialized RAGAS with Google Gemini (No OpenAI required)")

    def create_test_dataset(self) -> List[Dict]:
        """
        Create test dataset with questions and ground truth answers

        Returns:
            List of test cases with questions and ground truth
        """
        # Test dataset - customize based on your knowledge base
        test_data = [
            {
                "question": "‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏µ‡πà‡∏õ‡∏µ",
                "ground_truth": "‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô 5 ‡∏õ‡∏µ ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏ß‡∏¥‡∏†‡∏≤‡∏Ñ"
            },
            {
                "question": "‡∏à‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏≠‡∏∞‡πÑ‡∏£",
                "ground_truth": "‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï (‡∏ß‡∏®.‡∏ö.) ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤ ‡πÅ‡∏•‡∏∞‡πÉ‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û‡∏Ñ‡∏£‡∏π"
            },
            {
                "question": "‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£",
                "ground_truth": "‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤"
            },
            {
                "question": "‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ô‡∏µ‡πâ",
                "ground_truth": "‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏°‡∏±‡∏ò‡∏¢‡∏°‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡∏≠‡∏ô‡∏õ‡∏•‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡πà‡∏≤ ‡πÅ‡∏•‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏ö‡∏ö TCAS ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏á‡∏Ç‡∏≠‡∏á‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢"
            },
            {
                "question": "‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà",
                "ground_truth": "‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡∏£‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤"
            },
            {
                "question": "‡∏à‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á",
                "ground_truth": "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡πÇ‡∏¢‡∏ò‡∏≤ ‡∏Ñ‡∏£‡∏π‡∏™‡∏≠‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û‡∏î‡πâ‡∏≤‡∏ô‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏° ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤"
            },
            {
                "question": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏°‡∏µ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏µ‡πà‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï",
                "ground_truth": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏£‡∏ß‡∏°‡πÑ‡∏°‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 200 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏ß‡∏¥‡∏ä‡∏≤‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏™‡∏£‡∏µ"
            },
            {
                "question": "‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏≠‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà",
                "ground_truth": "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏≠‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ç‡∏≠‡∏á‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤"
            },
            {
                "question": "‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà",
                "ground_truth": "‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏ä‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå"
            },
            {
                "question": "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£",
                "ground_truth": "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ì‡∏∞‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢"
            }
        ]

        return test_data

    def generate_answers_and_contexts(self, test_data: List[Dict]) -> Dict[str, List]:
        """
        Generate answers and retrieve contexts for each question

        Args:
            test_data: List of test cases

        Returns:
            Dictionary with questions, answers, contexts, and ground truths
        """
        questions = []
        answers = []
        contexts = []
        ground_truths = []

        print("\n" + "="*80)
        print("üîç Generating answers and retrieving contexts...")
        print("="*80)

        for i, test_case in enumerate(test_data, 1):
            question = test_case["question"]
            ground_truth = test_case["ground_truth"]

            print(f"\n[{i}/{len(test_data)}] Processing: {question}")

            try:
                # Expand query for better retrieval
                expanded_query = self.chatbot.expand_query(question)

                # Get contexts from knowledge base
                relevant_knowledge = self.chatbot.knowledge_base.search_knowledge(
                    expanded_query,
                    n_results=5
                )

                # Extract context texts
                context_list = [item['text'] for item in relevant_knowledge]

                # Generate answer
                answer = self.chatbot.chat(question)

                # Store results
                questions.append(question)
                answers.append(answer)
                contexts.append(context_list)
                ground_truths.append(ground_truth)

                print(f"   ‚úÖ Answer: {answer[:80]}...")
                print(f"   üìö Retrieved {len(context_list)} contexts")

            except Exception as e:
                print(f"   ‚ùå Error: {e}")
                # Add placeholder to maintain consistency
                questions.append(question)
                answers.append("Error generating answer")
                contexts.append(["Error retrieving context"])
                ground_truths.append(ground_truth)

        return {
            "question": questions,
            "answer": answers,
            "contexts": contexts,
            "ground_truth": ground_truths
        }

    def evaluate(self, test_data: List[Dict] = None, use_all_metrics: bool = False) -> Dict:
        """
        Evaluate the chatbot using RAGAS metrics with Google Gemini

        Args:
            test_data: List of test cases (if None, use default dataset)
            use_all_metrics: Whether to use all metrics (requires more API calls)

        Returns:
            Evaluation results
        """
        # Use default dataset if none provided
        if test_data is None:
            test_data = self.create_test_dataset()

        # Generate answers and contexts
        data = self.generate_answers_and_contexts(test_data)

        # Create dataset for RAGAS
        dataset = Dataset.from_dict(data)

        # Select metrics to evaluate
        if use_all_metrics:
            metrics = [
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall,
            ]
            print("\nüìä Evaluating with all available RAGAS metrics...")
        else:
            # Use only the most important metrics to save API costs
            metrics = [
                faithfulness,
                answer_relevancy,
            ]
            print("\nüìä Evaluating with core RAGAS metrics (faithfulness, relevancy)...")

        print("‚è≥ This may take a few minutes...")
        print("ü§ñ Using Google Gemini for evaluation (No OpenAI required)")

        # Run evaluation with Gemini
        try:
            results = evaluate(
                dataset=dataset,
                metrics=metrics,
                llm=self.llm,
                embeddings=self.embeddings
            )

            return results

        except Exception as e:
            print(f"\n‚ùå Error during evaluation: {e}")
            print("\nüí° Troubleshooting tips:")
            print("   1. Check your GEMINI_API_KEY is valid")
            print("   2. Ensure you have internet connection")
            print("   3. Check Gemini API quota/limits")
            raise

    def save_results(self, results, output_file: str = "ragas_results_gemini.json"):
        """
        Save evaluation results to a JSON file

        Args:
            results: RAGAS evaluation results
            output_file: Output file path
        """
        # Convert results to dictionary
        results_dict = {
            "evaluator": "Google Gemini",
            "metrics": {k: float(v) for k, v in results.items()},
            "timestamp": pd.Timestamp.now().isoformat()
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)

        print(f"\nüíæ Results saved to {output_file}")

    def print_results(self, results):
        """
        Print evaluation results in a readable format

        Args:
            results: RAGAS evaluation results
        """
        print("\n" + "="*80)
        print("üìä RAGAS Evaluation Results (Using Google Gemini)")
        print("="*80)

        for metric, score in results.items():
            # Format metric name
            metric_name = metric.replace('_', ' ').title()

            # Interpret score
            if score >= 0.8:
                emoji = "üü¢"
                rating = "Excellent"
            elif score >= 0.6:
                emoji = "üü°"
                rating = "Good"
            elif score >= 0.4:
                emoji = "üü†"
                rating = "Fair"
            else:
                emoji = "üî¥"
                rating = "Needs Improvement"

            print(f"{emoji} {metric_name:25s}: {score:.4f} ({rating})")

        print("="*80)

        # Provide interpretation
        print("\nüìñ Metric Explanations:")
        print("-" * 80)
        print("‚Ä¢ Faithfulness: How factually accurate is the answer based on context? (1.0 = perfect)")
        print("‚Ä¢ Answer Relevancy: How relevant is the answer to the question? (1.0 = perfect)")
        print("‚Ä¢ Context Precision: How precise are the top-ranked contexts? (1.0 = perfect)")
        print("‚Ä¢ Context Recall: How well do contexts cover ground truth? (1.0 = perfect)")
        print("-" * 80)
        print("\nü§ñ Evaluation performed by: Google Gemini (No OpenAI required)")


def main():
    """Main function to run RAGAS evaluation with Gemini"""

    # Load environment variables
    load_dotenv()

    # Get API keys
    typhoon_api_key = os.getenv('TYPHOON_API_KEY')
    gemini_api_key = os.getenv('GEMINI_API_KEY')

    if not typhoon_api_key:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö TYPHOON_API_KEY ‡πÉ‡∏ô environment variables")
        print("üí° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° TYPHOON_API_KEY ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env")
        return

    if not gemini_api_key:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô environment variables")
        print("üí° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° GEMINI_API_KEY ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env")
        print("   ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà: https://ai.google.dev/")
        return

    try:
        # Initialize chatbot
        print("="*80)
        print("üîß Initializing chatbot...")
        print("="*80)

        kb = HybridKnowledgeBase(
            persist_directory="./chroma_db",
            collection_name="chatbot_knowledge",
            use_keyword_boost=True
        )
        chatbot = TyphoonChatbot(typhoon_api_key, kb)

        # Initialize evaluator with Gemini
        print("\nü§ñ Initializing RAGAS Evaluator with Google Gemini...")
        evaluator = RAGASEvaluatorGemini(chatbot, gemini_api_key)

        # Ask user about evaluation scope
        print("\n" + "="*80)
        print("‚öôÔ∏è  Evaluation Options")
        print("="*80)
        print("1. Core evaluation (faithfulness, relevancy) - Fast and economical")
        print("2. Full evaluation (all metrics) - More comprehensive")
        print("-"*80)

        choice = input("Choose evaluation mode (1 or 2) [default: 1]: ").strip()
        use_all_metrics = (choice == "2")

        # Run evaluation
        results = evaluator.evaluate(use_all_metrics=use_all_metrics)

        # Print results
        evaluator.print_results(results)

        # Save results
        evaluator.save_results(results)

        print("\n‚úÖ Evaluation completed successfully!")
        print("üéâ No OpenAI API required - powered by Google Gemini!")

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Evaluation interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
